import numpy as np
import pandas as pd
from scipy.stats import binom
# KNN
#from sklearn.neighbors import KNeighborsClassifier

# Logistic Regression
#from sklearn.linear_model import LogisticRegression


def rf_vectors(X_val, classifier):
    """
    Returns a DataFrame with class probability vectors generated by a Random Forest.

    Args:
        X_val (pd.DataFrame or array-like): Validation data
        classifier (RandomForestClassifier): Trained model

    Returns:
        pd.DataFrame: Probability vectors (columns=classes, rows=samples)
                     with original X_val indices preserved
    """
    # Get class probabilities
    y_proba = classifier.predict_proba(X_val)
    
    # Create DataFrame with class names as columns
    # and preserve original indices if X_val is a DataFrame
    index = X_val.index if hasattr(X_val, 'index') else None
    df_proba = pd.DataFrame(y_proba, 
                          columns=classifier.classes_, 
                          index=index)
    
    return df_proba

import numpy as np
import pandas as pd

def knn_vectors(X_val, classifier, return_mapping=False):
    """
    Transform X_val into a matrix with prediction vectors (neighbor class counts).
    
    Args:
        X_val: Validation features (pd.DataFrame or array-like)
        classifier: Fitted K-NN classifier (must have kneighbors method and _y + classes_ attributes)
        return_mapping: If True, returns (df_counts, class_to_num)
    
    Returns:
        pd.DataFrame or tuple
    """
    if not hasattr(classifier, '_y'):
        raise ValueError("Classifier must have stored training labels in _y attribute.")

    if any(label is None for label in classifier._y):
        raise ValueError("Training labels contain None values — clean your data.")

    # Nearest neighbors
    _, indices = classifier.kneighbors(X_val, n_neighbors=classifier.n_neighbors)

    # Get actual labels of neighbors
    neighbor_labels = classifier._y[indices]

    # Use unique labels from _y for mapping, not just classes_
    unique_labels = np.unique(classifier._y)
    class_to_num = {cls: i for i, cls in enumerate(unique_labels)}

    # Safe conversion of neighbor labels to numeric
    try:
        neighbor_numeric = np.vectorize(class_to_num.__getitem__)(neighbor_labels)
    except KeyError as e:
        raise KeyError(f"Label {e} not found in class_to_num mapping. Check label types.")

    # Count label occurrences
    counts_matrix = np.zeros((len(X_val), len(unique_labels)), dtype=int)
    for i, row in enumerate(neighbor_numeric):
        unique, counts = np.unique(row, return_counts=True)
        counts_matrix[i, unique] = counts

    # Create DataFrame
    df_counts = pd.DataFrame(
        counts_matrix,
        columns=unique_labels,
        index=X_val.index if hasattr(X_val, 'index') else range(len(X_val))
    )

    return (df_counts, class_to_num) if return_mapping else df_counts


"""
def knn_vectors(X_val, classifier, return_mapping=False):
    
    Transform X_val into a matrix with their respective prediction vectors (class neighbor counts)
    
    Args:
        X_val: Validation features (pd.DataFrame or array-like)
        classifier: Fitted K-NN classifier (must have kneighbors method and classes_ attribute)
        return_mapping: If True, returns a tuple (df_counts, class_to_num)
        
    Returns:
        pd.DataFrame or tuple: DataFrame with prediction vectors and optional mapping
    
    # Verify classifier has stored training labels
    if not hasattr(classifier, '_y'):
        raise ValueError("Classifier must have stored training labels (_y attribute)")
    
    # Check for None values in training labels
    if any(label is None for label in classifier._y):
        raise ValueError("Training labels contain None values - please clean your data first")
    
    # Get nearest neighbors indices
    _, indices = classifier.kneighbors(X_val, n_neighbors=classifier.n_neighbors)
    
    # Create numeric class mapping
    classes = classifier.classes_
    class_to_num = {cls: i for i, cls in enumerate(classes)}
    
    # Convert labels to numeric
    neighbor_labels = classifier._y[indices]
    neighbor_numeric = np.array([[class_to_num[cls] for cls in row] for row in neighbor_labels])
    
    # Count occurrences per row
    counts_matrix = np.zeros((len(X_val), len(classes)), dtype=int)
    for i, row in enumerate(neighbor_numeric):
        unique, counts = np.unique(row, return_counts=True)
        counts_matrix[i, unique] = counts
    
    # Create DataFrame
    df_counts = pd.DataFrame(
        counts_matrix,
        columns=classes,
        index=X_val.index if hasattr(X_val, 'index') else range(len(X_val))
    )
    
    return (df_counts, class_to_num) if return_mapping else df_counts
"""

def log_regression_vectors(X_val, classifier):
    """
    Transform X_val into a matrix with their respective prediction probabilities (vectors)
    
    Args:
        X_val: Validation features
        classifier: The logistic regression classifier
    
    Returns:
        A pandas DataFrame with prediction probability vectors (one column per class)
    """
    # Obtener las probabilidades de predicción para cada clase
    proba_matrix = classifier.predict_proba(X_val)
    
    # Obtener las clases del clasificador
    all_classes = classifier.classes_
    
    # Crear DataFrame con columnas como clases
    df_proba = pd.DataFrame(proba_matrix, columns=all_classes, index=X_val.index)
    
    return df_proba

def sorted_rf_numeric(X, y, classifier):
    """
    Generate a DataFrame with max probability, predicted class and true class,
    sorted by maximum probability (descending).

    Args:
        X: Input features (pd.DataFrame or array-like)
        y: True labels (pd.Series or array-like)
        classifier: Trained Random Forest classifier
        
    Returns:
        pd.DataFrame with columns:
        - 'max_prob': Maximum class probability
        - 'predicted': Predicted class (numeric index)
        - 'true': True class (numeric index)
        Sorted by 'max_prob' descending
    """
    # Get probability vectors (using modified rf_vectors without original_index)
    proba_df = rf_vectors(X, classifier)
    
    # Create class to index mapping
    class_to_idx = {cls: i for i, cls in enumerate(classifier.classes_)}
    
    # Convert y to Series if needed
    y_series = pd.Series(y) if not isinstance(y, pd.Series) else y
    
    # Create result DataFrame
    result_df = pd.DataFrame({
        'max_prob': proba_df.max(axis=1),
        'predicted_class': proba_df.idxmax(axis=1),
        'true_class': y_series
    })
    
    # Convert classes to numeric indices
    result_df['predicted'] = result_df['predicted_class'].map(class_to_idx)
    result_df['true'] = result_df['true_class'].map(class_to_idx)
    
    # Return sorted results (descending by probability)
    return result_df[['max_prob', 'predicted', 'true']].sort_values('max_prob', ascending=False)

def sorted_knn_numeric(X, y, classifier):
    """
    Generate a DataFrame with prediction probabilities, predicted class, and true class,
    sorted by maximum prediction probability.

    Args:
        X: Matrix with values to predict (pd.DataFrame or array-like)
        y: True labels for X (pd.Series or array-like)
        classifier: Fitted K-NN classifier (must have n_neighbors and classes_ attributes)
        
    Returns:
        pd.DataFrame: Sorted DataFrame with columns:
            - max_prob: Highest class probability
            - predicted: Predicted class (numeric index)
            - true: True class (numeric index)
    """
    # Convertir y a Series si es necesario
    y = pd.Series(y) if not isinstance(y, pd.Series) else y
    
    # Obtener vectores de predicción y normalizar por número de vecinos
    pred_probs = knn_vectors(X, classifier) / classifier.n_neighbors
    
    # Crear mapeo de clases a índices numéricos
    class_to_idx = {cls: i for i, cls in enumerate(classifier.classes_)}
    
    # Crear DataFrame con resultados
    result_df = pd.DataFrame({
        'max_prob': pred_probs.max(axis=1),
        'predicted_class': pred_probs.idxmax(axis=1),
        'true_class': y
    })
    
    # Convertir clases a índices numéricos
    result_df['predicted'] = result_df['predicted_class'].map(class_to_idx)
    result_df['true'] = result_df['true_class'].map(class_to_idx)
    
    # Eliminar columnas temporales y ordenar por probabilidad máxima
    return result_df[['max_prob', 'predicted', 'true']].sort_values('max_prob')

def sorted_logreg_numeric(X, y, classifier):
    """
    Generate a matrix with max probability value, prediction and true class of an instance
    
    Args:
        X: Matrix with values to predict
        y: Vector with true labels of X
        classifier: Logistic Regression classifier
        
    Returns:
        DataFrame sorted by prediction probability with columns:
        - max_prob: Highest class probability
        - predicted: Predicted class (numeric)
        - true: True class (numeric)
    """
    # Obtener probabilidades (softmax ya aplicado internamente en predict_proba)
    proba_df = classifier.predict_proba(X)
    
    # Mapeo de clases a valores numéricos
    map_class = {cls: i for i, cls in enumerate(classifier.classes_)}
    
    # Crear DataFrame con resultados
    result_df = pd.DataFrame({
        'max_prob': np.max(proba_df, axis=1),
        'predicted': classifier.classes_[np.argmax(proba_df, axis=1)],
        'true': y
    })
    
    # Convertir clases a valores numéricos usando el mapeo
    result_df['predicted'] = result_df['predicted'].map(map_class)
    result_df['true'] = result_df['true'].map(map_class)
    
    # Ordenar por probabilidad máxima
    return result_df.sort_values('max_prob')

def reject_predictions(X, thresh):
    """
    Identifies predictions with probability below class-specific thresholds.
    Works with any classifier that provides prediction probabilities in the required format.
    
    Args:
        X (pd.DataFrame): DataFrame containing:
            - 'max_prob': Maximum prediction probability (0-1)
            - 'predicted': Predicted class (matching thresh index)
        thresh (pd.Series/array-like): Threshold vector where index 
                                     corresponds to predicted classes
    
    Returns:
        pd.Series: Binary rejection vector (1=rejected, 0=accepted) 
                  with original indices
    """
    # Convert threshold to pandas Series if needed
    if not isinstance(thresh, pd.Series):
        thresh = pd.Series(thresh)
    
    # Verify all predicted classes have defined thresholds
    missing_classes = set(X['predicted']) - set(thresh.index)
    if missing_classes:
        raise ValueError(f"Undefined thresholds for classes: {missing_classes}")
    
    # Get the appropriate threshold for each prediction
    prediction_thresholds = thresh.loc[X['predicted']].values
    
    # Create rejection vector (1 where prob < threshold)
    return (X['max_prob'] <= prediction_thresholds).astype(int)

def calculate_metrics(y_true, y_pred, reject_vector):
    """
    Calcula coverage, accuracy en aceptados y efectividad de rechazo (rechazar errores).
    """
    y_true = y_true.loc[reject_vector.index]
    y_pred = y_pred.loc[reject_vector.index]

    accepted_mask = ~reject_vector.astype(bool)
    rejected_mask = reject_vector.astype(bool)

    coverage = 1 - reject_vector.mean()

    # Accuracy sobre aceptados: predicciones correctas entre los aceptados
    if accepted_mask.sum() > 0:
        select_accuracy = (y_pred[accepted_mask] == y_true[accepted_mask]).mean()
    else:
        select_accuracy = float('nan')

    # Reject accuracy: proporción de errores dentro de los rechazados
    if rejected_mask.sum() > 0:
        reject_accuracy = (y_pred[rejected_mask] != y_true[rejected_mask]).mean()
    else:
        reject_accuracy = float('nan')

    return coverage, select_accuracy, reject_accuracy

def compute_thresholds(X_val, y_val, delta, classifier):
    """
    Compute per-class rejection thresholds using binomial criterion for KNN
    
    Args:
        X_val: Validation features (pd.DataFrame or array-like)
        y_val: True labels for validation set (pd.Series or array-like)
        delta: Significance level for binomial test (float 0-1)
        classifier: Trained KNN classifier (must have kneighbors method)
        
    Returns:
        List[float]: Optimal thresholds (one per class)
    
    Note:
        Assumes classifier stores training labels internally (standard in scikit-learn)
    """
    # Verify classifier has stored training labels
    if not hasattr(classifier, '_y'):
        raise ValueError("Classifier must have stored training labels (_y attribute)")
    
    # Create class mapping
    classes = classifier.classes_
    class_to_idx = {cls: i for i, cls in enumerate(classes)}
    thresholds = [0.0] * len(classes)
    
    # Get predictions (using modified knn_vectors without y_train)
    prob_matrix = knn_vectors(X_val, classifier) / classifier.n_neighbors
    pred_classes = prob_matrix.idxmax(axis=1)
    
    # Create prediction DataFrame
    pred_df = pd.DataFrame({
        'max_prob': prob_matrix.max(axis=1),
        'predicted': pred_classes.map(class_to_idx),
        'true': pd.Series(y_val).map(class_to_idx)
    })
    
    # Compute thresholds per class
    for class_idx in range(len(classes)):
        class_preds = pred_df[pred_df["predicted"] == class_idx]
        misclassified = class_preds[class_preds["predicted"] != class_preds["true"]]
        
        if len(misclassified) == 0:
            continue
            
        best_thresh = 0.0
        best_sel_acc = 0.0
        
        for _, row in misclassified.iterrows():
            current_thresh = row["max_prob"]
            reject_region = class_preds[class_preds["max_prob"] < current_thresh]
            
            if len(reject_region) == 0:
                continue
                
            k = (reject_region["predicted"] == reject_region["true"]).sum()
            
            if binom.cdf(k, len(reject_region), 0.5) <= (1 - delta):
                selected = class_preds[class_preds["max_prob"] >= current_thresh]
                sel_acc = (selected["predicted"] == selected["true"]).mean()
                
                if sel_acc > best_sel_acc:
                    best_sel_acc = sel_acc
                    best_thresh = current_thresh
        
        thresholds[class_idx] = best_thresh
    
    return thresholds

def compute_thresholds_lr(X_val, y_val, delta, classifier):
    """
    Compute per-class rejection thresholds using binomial criterion for Logistic Regression
    
    Args:
        X_val: Validation features (pd.DataFrame or array-like)
        y_val: True labels for validation set (pd.Series or array-like)
        delta: Significance level for binomial test (float 0-1)
        classifier: Trained LogisticRegression classifier
        
    Returns:
        List[float]: Optimal thresholds (one per class)
    """
    # Create class mapping
    classes = classifier.classes_
    class_to_idx = {cls: i for i, cls in enumerate(classes)}
    thresholds = [0.0] * len(classes)
    
    # Get probability predictions
    prob_matrix = classifier.predict_proba(X_val)
    pred_classes = prob_matrix.argmax(axis=1)
    
    # Create prediction DataFrame
    pred_df = pd.DataFrame({
        'max_prob': prob_matrix.max(axis=1),
        'predicted': pred_classes,
        'true': pd.Series(y_val).map(class_to_idx)
    })
    
    # Compute thresholds per class
    for class_idx in range(len(classes)):
        class_preds = pred_df[pred_df["predicted"] == class_idx]
        misclassified = class_preds[class_preds["predicted"] != class_preds["true"]]
        
        if len(misclassified) == 0:
            continue
            
        best_thresh = 0.0
        best_sel_acc = 0.0
        
        for _, row in misclassified.iterrows():
            current_thresh = row["max_prob"]
            reject_region = class_preds[class_preds["max_prob"] < current_thresh]
            
            if len(reject_region) == 0:
                continue
                
            k = (reject_region["predicted"] == reject_region["true"]).sum()
            
            if binom.cdf(k, len(reject_region), 0.5) <= (1 - delta):
                selected = class_preds[class_preds["max_prob"] >= current_thresh]
                sel_acc = (selected["predicted"] == selected["true"]).mean()
                
                if sel_acc > best_sel_acc:
                    best_sel_acc = sel_acc
                    best_thresh = current_thresh
        
        thresholds[class_idx] = best_thresh
    
    return thresholds

def compute_thresholds_rf(X_val, y_val, delta, classifier):
    """
    Compute per-class rejection thresholds using binomial criterion for Random Forest
    
    Args:
        X_val: Validation features (pd.DataFrame or array-like)
        y_val: True labels for validation set (pd.Series or array-like)
        delta: Significance level for binomial test (float 0-1)
        classifier: Trained RandomForest classifier
        
    Returns:
        List[float]: Optimal thresholds (one per class)
    """
    # Create class mapping
    classes = classifier.classes_
    class_to_idx = {cls: i for i, cls in enumerate(classes)}
    thresholds = [0.0] * len(classes)
    
    # Get probability predictions
    prob_matrix = classifier.predict_proba(X_val)
    pred_classes = prob_matrix.argmax(axis=1)
    
    # Create prediction DataFrame
    pred_df = pd.DataFrame({
        'max_prob': prob_matrix.max(axis=1),
        'predicted': pred_classes,
        'true': pd.Series(y_val).map(class_to_idx)
    })
    
    # Compute thresholds per class
    for class_idx in range(len(classes)):
        class_preds = pred_df[pred_df["predicted"] == class_idx]
        misclassified = class_preds[class_preds["predicted"] != class_preds["true"]]
        
        if len(misclassified) == 0:
            continue
            
        best_thresh = 0.0
        best_sel_acc = 0.0
        
        for _, row in misclassified.iterrows():
            current_thresh = row["max_prob"]
            reject_region = class_preds[class_preds["max_prob"] < current_thresh]
            
            if len(reject_region) == 0:
                continue
                
            k = (reject_region["predicted"] == reject_region["true"]).sum()
            
            if binom.cdf(k, len(reject_region), 0.5) <= (1 - delta):
                selected = class_preds[class_preds["max_prob"] >= current_thresh]
                sel_acc = (selected["predicted"] == selected["true"]).mean()
                
                if sel_acc > best_sel_acc:
                    best_sel_acc = sel_acc
                    best_thresh = current_thresh
        
        thresholds[class_idx] = best_thresh
    
    return thresholds

def evaluate_knn_performance(X_test, y_test, classifier, thresholds_list=None):
    """
    Evaluate KNN classifier performance with different rejection thresholds
    
    Args:
        X_test: Test features (pd.DataFrame or array-like)
        y_test: True labels for test set (pd.Series or array-like)
        classifier: Trained KNN classifier
        thresholds_list: List of threshold sets to evaluate (default: [all 0s, all 1s])
    
    Returns:
        dict: Results for each threshold set with metrics
    """
    # Default thresholds: all 0s and all 1s
    if thresholds_list is None:
        n_classes = len(classifier.classes_)
        thresholds_list = [
            np.zeros(n_classes),  # Accept all predictions
            np.ones(n_classes)   # Reject all predictions
        ]
    
    # Get predictions (using updated sorted_knn_numeric without y_train)
    test_results = sorted_knn_numeric(X_test, y_test, classifier)
    
    # Evaluate each threshold set
    results = {}
    for i, thresholds in enumerate(thresholds_list):
        rejected = reject_predictions(test_results, thresholds)
        coverage, sel_acc, rej_acc = calculate_metrics(
            test_results["true"], 
            test_results["predicted"], 
            rejected
        )
        
        results[f'threshold_set_{i}'] = {
            'thresholds': thresholds,
            'coverage': coverage,
            'selected_accuracy': sel_acc,
            'rejected_accuracy': rej_acc
        }
        
        # Print results (maintaining original output format)
        print(f"\nThreshold Set {i}: {thresholds}")
        print(f"Coverage: {coverage:.4f}")
        print(f"Selected Accuracy: {sel_acc:.4f}")
        print(f"Rejected Accuracy: {rej_acc:.4f}")
    
    return results

def evaluate_logreg_performance(X_test, y_test, classifier, thresholds_list=None):
    """
    Evaluate Logistic Regression classifier performance with different rejection thresholds
    
    Args:
        X_test: Test features (pd.DataFrame or array-like)
        y_test: True labels for test set (pd.Series or array-like)
        classifier: Trained Logistic Regression classifier
        thresholds_list: List of threshold sets to evaluate 
                       (default: [all 0s - accept all, all 1s - reject all])
    
    Returns:
        dict: Results for each threshold set with metrics
    """
    # Default thresholds: all 0s (accept all) and all 1s (reject all)
    if thresholds_list is None:
        n_classes = len(classifier.classes_)
        thresholds_list = [
            np.zeros(n_classes),  # Accept all predictions
            np.ones(n_classes)    # Reject all predictions
        ]
    
    # Get predictions
    test_results = sorted_logreg_numeric(X_test, y_test, classifier)
    
    # Evaluate each threshold set
    results = {}
    for i, thresholds in enumerate(thresholds_list):
        rejected = reject_predictions(test_results, thresholds)
        coverage, sel_acc, rej_acc = calculate_metrics(
            test_results["true"], 
            test_results["predicted"], 
            rejected
        )
        
        # Store results
        results[f'threshold_set_{i}'] = {
            'thresholds': thresholds,
            'coverage': coverage,
            'selected_accuracy': sel_acc,
            'rejected_accuracy': rej_acc
        }
        
        # Print results in the original format
        print(f"Thresholds: {thresholds}")
        print(f"Coverage: {coverage}")
        print(f"Selected Accuracy: {sel_acc}")
        print(f"Rejected Accuracy: {rej_acc}\n")
    
    return results

def evaluate_rf_performance(X_test, y_test, classifier):
    """
    Evaluate Random Forest performance with different rejection thresholds.
    Uses the same format as evaluate_knn_performance and evaluate_logreg_performance.

    Args:
        X_test: Test features (pd.DataFrame or array-like)
        y_test: True labels for test set (pd.Series or array-like)
        classifier: Trained RandomForest classifier

    Returns:
        dict: Results for each threshold set with metrics
    """
    # Get predictions in standard format
    test_results = sorted_rf_numeric(X_test, y_test, classifier)
    
    # Create threshold sets (all 0s and all 1s)
    n_classes = len(classifier.classes_)
    thresholds_list = [
        np.zeros(n_classes),  # Accept all predictions
        np.ones(n_classes)    # Reject all predictions
    ]
    
    # Evaluate each threshold set
    results = {}
    for i, thresholds in enumerate(thresholds_list):
        rejected = reject_predictions(test_results, thresholds)
        coverage, sel_acc, rej_acc = calculate_metrics(
            test_results["true"], 
            test_results["predicted"], 
            rejected
        )
        
        # Store results
        results[f'threshold_set_{i}'] = {
            'thresholds': thresholds,
            'coverage': coverage,
            'selected_accuracy': sel_acc,
            'rejected_accuracy': rej_acc
        }
        
        # Print results in standard format
        print(f"\nThresholds: {thresholds}")
        print(f"Coverage: {coverage:.4f}")
        print(f"Selected Accuracy: {sel_acc:.4f}" if not np.isnan(sel_acc) else "Selected Accuracy: N/A")
        print(f"Rejected Accuracy: {rej_acc:.4f}" if not np.isnan(rej_acc) else "Rejected Accuracy: N/A")
    
    return results